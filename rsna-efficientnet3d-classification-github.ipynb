{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Acknowledgements: <br>\nhttps://www.kaggle.com/ohbewise/a-rsna-mri-solution-from-dicom-to-submission <br>\nU.Baid, et al., “The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification”, arXiv:2107.02314, 2021.","metadata":{}},{"cell_type":"markdown","source":"## Install and import libraries","metadata":{}},{"cell_type":"code","source":"# If this line fails please see the prerequisite above\n!pip install --quiet --no-index --find-links ../input/pip-download-torchio/ --requirement ../input/pip-download-torchio/requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import libraries\nimport os\nimport csv\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport nibabel as nib\nimport torchio as tio\nimport tensorflow as tf\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n# Parameters to limit the processing power needed.\nscan_types    = ['FLAIR','T2w'] # uses all scan types","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess data: DICOM to normalized NifiTi with TorchIO\n","metadata":{}},{"cell_type":"code","source":"# Preprocess data \ndata_dir   = '/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/'\nout_dir    = '/kaggle/working/processed'\n\nfor dataset in ['train']:\n    dataset_dir = f'{data_dir}{dataset}'\n    patients = os.listdir(dataset_dir)\n    \n    # Remove cases the competion host said to exclude \n    # https://www.kaggle.com/c/rsna-miccai-brain-tumor-radiogenomic-classification/discussion/262046\n    if '00109' in patients: patients.remove('00109')\n    if '00123' in patients: patients.remove('00123')\n    if '00709' in patients: patients.remove('00709')\n    \n    print(f'Total patients in {dataset} dataset: {len(patients)}')\n\n    count = 0\n    for patient in patients:\n        count = count + 1\n        print(f'{dataset}: {count}/{len(patients)}')\n\n        for scan_type in scan_types:\n            scan_src  = f'{dataset_dir}/{patient}/{scan_type}/'\n            scan_dest = f'{out_dir}/{dataset}/{patient}/{scan_type}/'\n            Path(scan_dest).mkdir(parents=True, exist_ok=True)\n            image = tio.ScalarImage(scan_src)\n            transforms = [\n                tio.ToCanonical(),\n                tio.Resample(1),\n                tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n                tio.CropOrPad((128,128,64)),\n            ]\n            transform = tio.Compose(transforms)\n            preprocessed = transform(image)\n            preprocessed.save(f'{scan_dest}/{scan_type}.nii.gz')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build datasets: NifiTi to Split Dataset with NiBabel","metadata":{}},{"cell_type":"code","source":"# build datasets\n\n# dataset processing functions\ndef read_nifti_file(filepath):\n    \"\"\"Read and load volume\"\"\"\n    # Read file\n    scan = nib.load(filepath)\n    # Get raw data\n    scan = scan.get_fdata()\n    return scan\n\ndef add_batch_channel(volume):\n    \"\"\"Process validation data by adding a channel.\"\"\"\n    volume = tf.expand_dims(volume, axis=-1)\n    volume = tf.expand_dims(volume, axis=0)\n    return volume\n\ndef process_scan(filepath):\n    scan = read_nifti_file(filepath)\n    volume = add_batch_channel(scan)\n    return volume\n\n# get labels\nlabels_df = pd.read_csv(data_dir+'train_labels.csv', index_col=0)\n\n# split patients\npatients = os.listdir(f'{out_dir}/train')\nfrom sklearn.model_selection import train_test_split\ntrain, validation = train_test_split(patients, test_size=0.3, random_state=42)\nprint(f'{len(patients)} total patients.\\n   {len(train)} in the train split.\\n   {len(validation)} in the validation split')\n\nsplits_dict = {'train':train, 'validation':validation}\n\nfor scan_type in scan_types:\n    print(f'{scan_type} start')\n    for split_name, split_list in splits_dict.items():\n        print(f'   {split_name} start')\n        label_list = []\n        filepaths = []\n        for patient in split_list:\n            label = labels_df._get_value(int(patient), 'MGMT_value')\n            label = add_batch_channel(label)\n            label_list.append(label)\n            filepath  = f'{out_dir}/train/{patient}/{scan_type}/{scan_type}.nii.gz'\n            filepaths.append(filepath)\n\n        features = np.array([process_scan(filepath) for filepath in filepaths if filepath])\n        labels = np.array(label_list, dtype=np.uint8)\n        dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n        \n        # save dataset   \n        tf_data_path = f'./datasets/{scan_type}_{split_name}_dataset'\n        tf.data.experimental.save(dataset, tf_data_path, compression='GZIP')\n        with open(tf_data_path + '/element_spec', 'wb') as out_:  # also save the element_spec to disk for future loading\n            pickle.dump(dataset.element_spec, out_)\n        print(f'   {split_name} done')\n    print(f'{scan_type} done')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define, train, and evaluate model:  Dataset to Model with Tensorflow\n","metadata":{}},{"cell_type":"markdown","source":"train test split:","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(f\"{data_directory}/train_labels.csv\")\ndisplay(train_df)\n\ndf_train, df_valid = sk_model_selection.train_test_split(\n    train_df, \n    test_size=0.2, \n    random_state=12, \n    stratify=train_df[\"MGMT_value\"],\n)\ndf_train.tail()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"model definition and training:","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = EfficientNet3D.from_name(\"efficientnet-b7\", override_params={'num_classes': 2}, in_channels=1)\n        n_features = self.net._fc.in_features\n        self.net._fc = nn.Linear(in_features=n_features, out_features=1, bias=True)\n    \n    def forward(self, x):\n        out = self.net(x)\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Trainer:\n    def __init__(\n        self, \n        model, \n        device, \n        optimizer, \n        criterion\n    ):\n        self.model = model\n        self.device = device\n        self.optimizer = optimizer\n        self.criterion = criterion\n\n        self.best_valid_score = np.inf\n        self.n_patience = 0\n        self.lastmodel = None\n        \n    def fit(self, epochs, train_loader, valid_loader, save_path, patience):        \n        for n_epoch in range(1, epochs + 1):\n            self.info_message(\"EPOCH: {}\", n_epoch)\n            \n            train_loss, train_time = self.train_epoch(train_loader)\n            valid_loss, valid_auc, valid_time = self.valid_epoch(valid_loader)\n            \n            self.info_message(\n                \"[Epoch Train: {}] loss: {:.4f}, time: {:.2f} s            \",\n                n_epoch, train_loss, train_time\n            )\n            \n            self.info_message(\n                \"[Epoch Valid: {}] loss: {:.4f}, auc: {:.4f}, time: {:.2f} s\",\n                n_epoch, valid_loss, valid_auc, valid_time\n            )\n\n            # if True:\n            #if self.best_valid_score < valid_auc: \n            if self.best_valid_score > valid_loss: \n                self.save_model(n_epoch, save_path, valid_loss, valid_auc)\n                self.info_message(\n                     \"auc improved from {:.4f} to {:.4f}. Saved model to '{}'\", \n                    self.best_valid_score, valid_loss, self.lastmodel\n                )\n                self.best_valid_score = valid_loss\n                self.n_patience = 0\n            else:\n                self.n_patience += 1\n            \n            if self.n_patience >= patience:\n                self.info_message(\"\\nValid auc didn't improve last {} epochs.\", patience)\n                break\n            \n    def train_epoch(self, train_loader):\n        self.model.train()\n        t = time.time()\n        sum_loss = 0\n\n        for step, batch in enumerate(train_loader, 1):\n            X = batch[\"X\"].to(self.device)\n            targets = batch[\"y\"].to(self.device)\n            self.optimizer.zero_grad()\n            outputs = self.model(X).squeeze(1)\n            \n            loss = self.criterion(outputs, targets)\n            loss.backward()\n\n            sum_loss += loss.detach().item()\n\n            self.optimizer.step()\n            \n            message = 'Train Step {}/{}, train_loss: {:.4f}'\n            self.info_message(message, step, len(train_loader), sum_loss/step, end=\"\\r\")\n        \n        return sum_loss/len(train_loader), int(time.time() - t)\n    \n    def valid_epoch(self, valid_loader):\n        self.model.eval()\n        t = time.time()\n        sum_loss = 0\n        y_all = []\n        outputs_all = []\n\n        for step, batch in enumerate(valid_loader, 1):\n            with torch.no_grad():\n                X = batch[\"X\"].to(self.device)\n                targets = batch[\"y\"].to(self.device)\n\n                outputs = self.model(X).squeeze(1)\n                loss = self.criterion(outputs, targets)\n\n                sum_loss += loss.detach().item()\n                y_all.extend(batch[\"y\"].tolist())\n                outputs_all.extend(torch.sigmoid(outputs).tolist())\n\n            message = 'Valid Step {}/{}, valid_loss: {:.4f}'\n            self.info_message(message, step, len(valid_loader), sum_loss/step, end=\"\\r\")\n            \n        y_all = [1 if x > 0.5 else 0 for x in y_all]\n        auc = roc_auc_score(y_all, outputs_all)\n        \n        return sum_loss/len(valid_loader), auc, int(time.time() - t)\n    \n    def save_model(self, n_epoch, save_path, loss, auc):\n        self.lastmodel = f\"{save_path}-e{n_epoch}-loss{loss:.3f}-auc{auc:.3f}.pth\"\n        torch.save(\n            {\n                \"model_state_dict\": self.model.state_dict(),\n                \"optimizer_state_dict\": self.optimizer.state_dict(),\n                \"best_valid_score\": self.best_valid_score,\n                \"n_epoch\": n_epoch,\n            },\n            self.lastmodel,\n        )\n    \n    @staticmethod\n    def info_message(message, *args, end=\"\\n\"):\n        print(message.format(*args), end=end)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef train_mri_type(df_train, df_valid, mri_type):\n    if mri_type==\"all\":\n        train_list = []\n        valid_list = []\n        for mri_type in mri_types:\n            df_train.loc[:,\"MRI_Type\"] = mri_type\n            train_list.append(df_train.copy())\n            df_valid.loc[:,\"MRI_Type\"] = mri_type\n            valid_list.append(df_valid.copy())\n\n        df_train = pd.concat(train_list)\n        df_valid = pd.concat(valid_list)\n    else:\n        df_train.loc[:,\"MRI_Type\"] = mri_type\n        df_valid.loc[:,\"MRI_Type\"] = mri_type\n\n    print(df_train.shape, df_valid.shape)\n    display(df_train.head())\n    \n    train_data_retriever = Dataset(\n        df_train[\"BraTS21ID\"].values, \n        df_train[\"MGMT_value\"].values, \n        df_train[\"MRI_Type\"].values,\n        augment=True\n    )\n\n    valid_data_retriever = Dataset(\n        df_valid[\"BraTS21ID\"].values, \n        df_valid[\"MGMT_value\"].values,\n        df_valid[\"MRI_Type\"].values\n    )\n\n    train_loader = torch_data.DataLoader(\n        train_data_retriever,\n        batch_size=4,\n        shuffle=True,\n        num_workers=8,pin_memory = True\n    )\n\n    valid_loader = torch_data.DataLoader(\n        valid_data_retriever, \n        batch_size=4,\n        shuffle=False,\n        num_workers=8,pin_memory = True\n    )\n\n    model = Model()\n    model.to(device)\n\n    #checkpoint = torch.load(\"best-model-all-auc0.555.pth\")\n    #model.load_state_dict(checkpoint[\"model_state_dict\"])\n\n    #print(model)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    #optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n    criterion = torch_functional.binary_cross_entropy_with_logits\n\n    trainer = Trainer(\n        model, \n        device, \n        optimizer, \n        criterion\n    )\n\n    history = trainer.fit(\n        10, \n        train_loader, \n        valid_loader, \n        f\"{mri_type}\", \n        10,\n    )\n    \n    return trainer.lastmodel\n\nmodelfiles = None\n\nif not modelfiles:\n    modelfiles = [train_mri_type(df_train, df_valid, m) for m in mri_types]\n    print(modelfiles)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Write predictions to submission.csv: Model Prediction to Submission","metadata":{}},{"cell_type":"code","source":"# write predictions to submission.csv\n\n# Set up directories\ndata_dir   = '/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/'\ntest_dir   = f'{data_dir}test'\npatients = os.listdir(test_dir)\nif demo:\n    patients = patients[:10]\nprint(f'Total patients: {len(patients)}\\n\\n')\n\nout_dir    = '/kaggle/working/processed'\n\nscan_types = ['FLAIR', 'T2w']\n\nfor scan_type in scan_types:\n    f = open(f'/kaggle/working/submission.csv', 'w')\n    writer = csv.writer(f)\n    writer.writerow(['BraTS21ID','MGMT_value'])\n    for patient in patients:\n        # dicom to nifiti\n        scan_src  = f'{test_dir}/{patient}/{scan_type}/'\n        scan_dest = f'{out_dir}/test/{patient}/{scan_type}/'\n        Path(scan_dest).mkdir(parents=True, exist_ok=True)\n        image = tio.ScalarImage(scan_src)  # subclass of Image\n        transforms = [\n            tio.ToCanonical(),\n            tio.Resample(1),\n            tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n            tio.CropOrPad((128,128,64)),\n        ]\n        transform = tio.Compose(transforms)\n        preprocessed = transform(image)\n        filepath = f'{scan_dest}/{scan_type}.nii.gz'\n        preprocessed.save(filepath)\n        \n        # process_scan\n        case = process_scan(filepath)\n\n        # tf model\n        model = tf.keras.models.load_model(f'./models/{scan_type}')\n\n        # get prediction\n        prediction = model.predict(case)\n        \n        # write prediction\n        print(f'{patient},{prediction[0][0]}')\n        writer.writerow([patient, prediction[0][0]])\n\n    f.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}